\documentclass[a4paper]{scrartcl}

% Text
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{csquotes}

% Math
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage{bm}
	\newcommand{\vv}[1]{\ensuremath{\bm{#1}}}
	\newcommand{\mat}[1]{\ensuremath{\bm{#1}}}

    \newcommand{\T}[1]{\ensuremath{{#1}^{T}}}

	\newcommand\dd{\ensuremath{\:d}}

	\newcommand\R{\ensuremath{\mathbb{R}}}
	\newcommand\Z{\ensuremath{\mathbb{Z}}}
	\DeclareMathOperator\supp{supp}
	\DeclareMathOperator\diag{diag}
	\DeclareMathOperator\tr{tr}

% Set
% \given is part of the syntax, make sure it exists
% \providecommand\given{}
\newcommand\given{\:\vert\:}
\newcommand\SetSymbol[1][]{
	\nonscript\: #1 \vert \nonscript\:
	\mathopen{} % make - etc behave as sign
	\allowbreak % in text, allow line break after the \given
}
\DeclarePairedDelimiterX\Set[1]{\{}{\}}{
	% Change is local to \Set
	\renewcommand\given{\SetSymbol[\delimsize]}
	#1
}

\usepackage{graphicx}
\usepackage{asymptote}

\usepackage[colorlinks=true]{hyperref}

\usepackage{cleveref}

\usepackage{biblatex}
\bibliography{literature.bib}

\title{Smoothing Splines}
\author{Robert Dahl Jacobsen}

\begin{document}

\maketitle

Here I document the math used in the Julia package \textit{SmoothSpline} that performs regression with B-splines using a Tikhonov regularisation.

The \textit{SmoothSpline} package is a port of the \textit{smooth.spline} function from R.
During the implementation I discovered two special things in \textit{smooth.spline}'s implementation that I have not seen in its documentation:

\begin{itemize}
    \item Two matrix traces are computed. In the code the first two and last two entries on the diagonal are \emph{not} included in the trace. 
    \item In one computation $\tfrac13$ is hard-coded to be $0.333$. This approximation is a bit crude when the word size of the computer is 64 bit.
\end{itemize}

I will comment more on these points later in this document.


\section{B-splines}
\label{sec:bsplines}

My source for B-splines is the excellent book \cite{Piegl:Tiller:1997} with very detailed algorithms.
The only downside for a Julia implementation is that all algorithms use 0-based arrays.
I do not include further details about about evaluation of B-splines here, but note that we have algorithms for computing the values of B-splines and all of their derivatives.


In \textit{SmoothSpline} we use cubic B-splines, so the general degree $p$ from \cite{Piegl:Tiller:1997} is always $p = 3$.
We therefore use a more compact notation here:
The $i$'th spline of degree $p$ is in \cite{Piegl:Tiller:1997} denoted $N_{i,p}$, but is here simply denoted $N_i$.
However, to avoid \enquote{magic} occurences of 3, I still use $p$ in the following to denote the degree.

A collection of B-splines are determined solely by their knots.
Boundaries are handled by reusing the boundary knots:
If we have $m$ distinct breakpoints $u_1, \ldots, u_m$ we construct the B-splines from the knots, where we have augmented by $p$ endpoints in each end ($u_1$ and $u_m$ repeated $p$ times each).

The function \textit{bs} from the \{splines\} package in R can be used to compute B-splines.


\section{Regression with B-splines}
\label{sec:regression}

We consider observations $(x_i, y_i)$ for $i = 1, \ldots, n$. 
It is allowed to have multiple observations with similar $x$ values (and $y$ values).
Observation $i$ has an associated weight $w_i$, that by default is $1$.
Observations with similar $x$ values are grouped by summing their weights and taking the average of thier $y$ values.
In the following we assume that all $x$ values are distinct and that the observations are sorted by their $x$ values.

The choice of B-splines (that is, the choice of their knots) of course influences everything.
In \textit{smooth.spline} the number of internal knots $m$ is determined from the number of observations $n$.
Let
\begin{equation*}
    a_1 = \log_2(50),
    \quad
    a_2 = \log_2(100),
    \quad
    a_3 = \log_2(140),
    \quad
    a_4 = \log_2(200)
\end{equation*}
and
\begin{equation*}
    m' =
    \begin{dcases}
        n, & n < 50,
        \\
        2^{a_1 + (a_2 - a_1) (n - 50) / 150}, & 50 \leq n < 200,
        \\
        2^{a_2 + (a_3 - a_2) (n - 200) / 600}, & 200 \leq n < 800,
        \\
        2^{a_3 + (a_4 - a_3) (n - 800) / 2400}, & 800 \leq n < 3200,
        \\
        200 + (n - 3200)^{0.2}, & n \geq 3200.
    \end{dcases}
\end{equation*}
Then $m = [m']$.
The breakpoints $u_1, \dots, u_m$ are computed as
\begin{equation*}
    u_i = \Bigl[\frac{u_m - u_1}{m - 1} (i - 1) + u_i\Bigr].
\end{equation*}

We want to perform regression with $m$ B-splines on the interval $[x_1, x_n]$.
That is, compute an approximation with the function
\begin{align*}
    f(x) = \sum_{j=1}^M \beta_j N_j(x).
\end{align*}

The design matrix of the regression problem is the matrix $\mat X$ of size $n\times m$ with entries
\begin{equation*}
    X_{i,j} = N_j(u_i).
\end{equation*}

The weights are collected in a diagonal matrix $\mat W = \diag(w_i, i = 1, \ldots, n)$.
To enforce a smoother interpolant we choose the $\beta$'s with a least squares criterion and limit the curvature (measured by the second derivative):
\begin{equation*}
    \min_{\vv \beta} \|\vv y - \mat X \sqrt{\mat W} \vv \beta\|^2 + \lambda \int_{x_1}^{x_n} \bigl\{f^{(2)}(t)\bigr\}^2 \dd t.
\end{equation*}

We can also write this as a Tikhonov regularisation
\begin{equation*}
    \min_{\vv \beta} \|\vv y - \mat X \sqrt{\mat W} \vv \beta\| + \lambda \T{\vv\beta} \mat\Sigma \vv\beta,
\end{equation*}

where the regularisation term $\mat\Sigma$ is the Gram matrix of size $m\times m$ with entries
\begin{equation*}
    \Sigma_{i,j} =
    \int_{x_1}^{x_n} N_i^{(2)}(t) N_j^{(2)}(t) \dd t.
\end{equation*}

For each value of $\lambda > 0$, we have a solution $\vv\beta$ to the corresponding regression problem, namely the solution to the equation
\begin{equation}
    \label{eq:normal_equation}
    \mat A \vv \beta
    = \bigl(\T{\mat X} \mat W \mat X + \lambda \mat\Sigma\bigr) \vv\beta
    = \T{\mat X} \mat W \vv y 
    = \widetilde{\vv y}.
\end{equation}

This can be solved fast with a few tricks.
Both $\mat X$ and $\mat\Sigma$ are \textit{banded} with lower and upper band $p$.
That is, $\mat X_{i,j} = \mat\Sigma_{i,j} = 0$ when $|i - j| > p$.
This implies that the Cholesky factorization of $\mat A$ is also banded with the same band.
More specifically, we let $\mat C$ denote the upper triangular matrix of the Cholesky factorization such that 
\begin{equation*}
    \T{\mat C} \mat C = \T{\mat X} \mat W \mat X + \lambda \mat\Sigma.
\end{equation*}

With this convention $\T{\mat C}$ is a lower triangular matrix and we can the use the classic forward and back substitution to solve \cref{eq:normal_equation}:
First solve $\T{\mat C} \vv z = \widetilde{\vv y}$ and then $\mat C \vv \beta = \vv z$.

Note that the matrix $\mat A$ may be ill-conditioned even with the Tikhonov term.
In fact, I have seen substantial differences between $\vv\beta$'s computed with generic solvers and the special solvers used in \textit{SmoothSpline}.


\subsection{Computing matrices}

Before solving \cref{eq:normal_equation} we must compute the matrices involved.

In the \textit{documentation} for \textit{smooth.spline} (and therefore not under the same license as the \textit{code}) it is noted that the Lagrange multiplier $\lambda$ is data dependent.
That is, an appropriate value for $\lambda$ depends on the values of the observations.
This makes it more delicate to choose $\lambda$.

But \textit{smooth.spline} aids the user by offering a data independent parameter \enquote{spar}, where $0 < \text{spar} \leq 1$, that is mapped to an appropriate $\lambda$.
The map is
\begin{equation*}
    \lambda = r\cdot 256^{3\cdot\text{spar} - 1},
    \quad
    r = \frac{\tr(\T{\mat X} \mat W \mat X)}{\tr(\mat\Sigma)}.
\end{equation*}

In the code, however, it is not the standard traces that are computed -- both in \textit{SmoothSpline} and \textit{smooth.spline}.
The first two entries and the last two entries of the diagonals are discarded in the sum.
I have not found documentation to explain this, but I \emph{suspect} that it is to reduce the impact of the endpoint knots that occur in multiple splines -- cf. \cref{sec:bsplines}.

Furthermore, \textit{smooth.spline} rescales the $x$ values to the closed unit interval.
This has an impact on $\mat\Sigma$, but the impact on $\mat A$ is de facto alleviated by the above mapping.


\subsubsection{Design matrix}

The design matrix to compute is straightforward once we know how to compute the splines.


\subsubsection{Gram matrix}

We have closed-form expressions for the Tikhonov matrix $\mat\Sigma$.
All spline functions are supported on (a subset of) $[u_0, u_m]$ and we first note that
\begin{equation}
    \label{eq:sigma_entry}
    \Sigma_{i,j}
    = \int_{u_0}^{u_m} N_i^{(2)}(t) N_j^{(2)}(t) \dd t
    = \sum_{k = 0}^{m - 1} \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t.
\end{equation}

With partial integration each of the integrals can be computed as

\begin{equation*}
    \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t
    = \Bigl[ N_i'(t) N_j^{(2)}(t) \Bigr]_{u_k}^{u_{k+1}} -
    \int_{u_k}^{u_{k+1}} N_i'(t) N_j^{(3)}(t) \dd t.
\end{equation*}

On each interval $[u_k, u_{k+1}]$, every spline is a polynomial of degree at most three.
Hence the third derivative is a constant and

\begin{equation*}
    \int_{u_k}^{u_{k+1}} N_i'(t) N_j^{(3)}(t) \dd t
    = N_j^{(3)}(u_k) \bigl(N_i(u_{k+1}) - N_i(u_k)\bigr).
\end{equation*}

Substituting all this into \cref{eq:sigma_entry} we note a telescoping sum and arrive at an expression:
\begin{equation*}
    \Sigma_{i,j}
    = N_i'(u_m) N_j^{(2)}(u_m) - N_i'(u_0) N_j^{(2)}(u_0) 
    - \sum_{k = 0}^{m - 1} N_j^{(3)}(u_k) \bigl(N_i(u_{k+1}) - N_i(u_k)\bigr).
\end{equation*}

In \textit{smooth.spline} the integral in \cref{eq:sigma_entry} is computed differently:
On each interval $[u_k, u_{k+1}]$, the function $N_i^{(2)}$ is a polynomial of degree at most one.
In the code, this polynomial is parameterized as

\begin{equation*}
    N_i^{(2)}(t) 
    = a_{i,k} + b_{i,k} (t - u_k), 
    \quad u_k\leq t\leq u_{k+1}.
\end{equation*}

Expanding parentheses in $N_i^{(2)} N_j^{(2)}$ we see that

\begin{equation*}
    N_i^{(2)}(t) N_j^{(2)}(t)
    = a_{i,k} a_{j,k} + \bigl(a_{i,k} b_{j,k} + a_{j,k} b_{i,k}\bigr) (t - u_k) + b_{i,k} b_{j,k} (t - u_k)^2.
\end{equation*}

With this expression and $\Delta_k = u_{k+1} - u_k$,

\begin{align}
    \MoveEqLeft
    \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t 
    \nonumber
    \\
    & = \int_0^{\Delta_k} a_{i,k} a_{j,k} + \bigl(a_{i,k} b_{j,k} + a_{j,k} b_{i,k}\bigr) s + b_{i,k} b_{j,k} s^2 \dd s 
    \nonumber
    \\
    & = a_{i,k} a_{j,k} \Delta_k + \bigl(a_{i,k} b_{j,k} + a_{j,k} b_{i,k}\bigr) \frac12 \Delta_k^2 + b_{i,k} b_{j,k} \frac13 \Delta_k^3.
    \label{eq:smooth.spline_integral}
\end{align}

The slope $b_{i,k} = N_i^{(3)}(t)$, but \textit{smooth.spline} only use the second order derivatives.
We see readily that $a_{i,k} = N_i^{(2)}(u_k)$.
Let $\Delta y_{i,k} = N_i^{(2)}(u_{k+1}) - N_i^{(2)}(u_k)$.
Then $b_{i,k} = \Delta y_{i,k} / \Delta_k$.
With these expressions, many of the $\Delta_k$'s can be removed from \cref{eq:smooth.spline_integral}:

\begin{align*}
    \MoveEqLeft
    \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t 
    \\
    & = a_{i,k} a_{j,k} \Delta_k + \Bigl(a_{i,k} \frac{\Delta y_{j,k}}{\Delta_k} + a_{j,k} \frac{\Delta y_{i,k}}{\Delta_k}\Bigr) \frac12 \Delta_k^2 + \frac{\Delta y_{i,k}}{\Delta_k} \frac{\Delta y_{j,k}}{\Delta_k} \frac13 \Delta_k^3
    \\
    & = a_{i,k} a_{j,k} \Delta_k + \bigl(a_{i,k} \Delta y_{j,k} + a_{j,k} \Delta y_{i,k}\bigr) \frac12 \Delta_k + \Delta y_{i,k} \Delta y_{j,k} \frac13 \Delta_k.
\end{align*}

In \textit{smooth.spline} the value of $\tfrac13$ is hard-coded to be $0.333$.
Even in small examples this can result in entry-wise deviations of about $1\%$.


\paragraph{Speed-ups}

From the definition we readily see that $\mat\Sigma$ is symmetric.
We know from \cite[P2.1]{Piegl:Tiller:1997} that the support of $N_i$ is $[u_i, u_{i+p+1}]$ and therefore 
\begin{equation*}
    \supp N_i \cap \supp N_j = \emptyset
    \quad\text{if}\:
    |i - j| > p.
\end{equation*}
This implies that $\Sigma_{i,j} = 0$ if $|i - j| > p$ and that the sum in \cref{eq:sigma_entry} goes from $\min\{i, j\}$ to $\min\{\max\{i, j\} + p + 1, m\}$.


\subsection{Banded matrices}

A banded matrix can be represented and stored efficiently \cite[Section 1.2.5]{Golub:van_Loan:2013}.
Furthermore, there are efficient algorithms for computing the Cholesky factorization of a banded matrix \cite[Section 4.3]{Golub:van_Loan:2013}.

In R, \textit{smooth.spline} rely on the Fortran routines \textit{dpbfa} and \textit{dpbsl} from Linpack \cite{Dongarra:Moler:Bunch:Stewart:1979} to compute the Cholesky factorization and solving \cref{eq:normal_equation}, respectively.


\printbibliography

\end{document}

